{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ffe31e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\athul raj nambiar\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\scipy\\__init__.py:173: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.18.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Requires most of the same imports as for few-shot NER training\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaModel\n",
    "from transformers import BertTokenizer, pipeline\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from sample_few_shot import get_label_dict\n",
    "from finetune_model import RobertaNER, BertNER\n",
    "from data import *\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c75f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# Checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720ab528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting data to be able to input it, into the model\n",
    "def generate_batch(batch):\n",
    "    text = [F.pad(torch.tensor(x[0]), (0,max_seq_len-len(x[0])), \"constant\", 1) for x in batch] # batch_size * max_seq_len \n",
    "    text = pad_sequence(text, batch_first = True)\n",
    "    attention_mask = [torch.cat((torch.ones_like(torch.tensor(x[0])), torch.zeros(max_seq_len-len(x[0]), dtype=torch.int64)), dim=0)\n",
    "        if len(x[0]) < max_seq_len else torch.ones_like(torch.tensor(x[0]))[:max_seq_len] for x in batch]\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first = True)\n",
    "    label = [F.pad(torch.tensor(x[1]), (0,max_seq_len-len(x[1])), \"constant\", -100) for x in batch]\n",
    "    label = pad_sequence(label, batch_first = True)\n",
    "    orig_len = [len(x[0]) for x in batch]\n",
    "\n",
    "    return text, attention_mask, label, orig_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e19f817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class with a lot of the information that we use hyper parameters and file names\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        i = 0\n",
    "        self.datapath = 'dataset'\n",
    "        self.dataset = 'custom'\n",
    "        self.train_text = f'FS_train_dataset{i}.words'\n",
    "        self.train_ner = f'FS_train_dataset{i}.ner'\n",
    "        self.test_text = f'FS_test_dataset{i}.words'\n",
    "        self.test_ner = f'FS_test_dataset{i}.ner'\n",
    "        self.model_save_name = f'FS_train_dataset{i}_finetuned_model'\n",
    "        self.few_shot_sets = 1\n",
    "        self.unsup_text = None\n",
    "        self.unsup_ner = None\n",
    "        self.base_model = 'roberta'\n",
    "        self.epoch = 5\n",
    "        self.train_cls_num = 4\n",
    "        self.test_cls_num = 18\n",
    "        self.max_seq_len = 128\n",
    "        self.batch_size = 8\n",
    "        self.soft_kmeans = False\n",
    "        self.lr = 1e-04\n",
    "        self.unsup_lr = 0.5\n",
    "        self.warmup_proportion = 0.1\n",
    "        self.weight_decay = 0.01\n",
    "        self.use_truecase = False\n",
    "        self.local_rank = None\n",
    "        self.use_gpu = 'cuda'\n",
    "        self.data_size = ''\n",
    "        self.load_model = True\n",
    "        self.reinit = False\n",
    "        self.load_model_name = 'pretrained_models/lc_pretrained_190.pt'\n",
    "        self.load_checkpoint = False\n",
    "        self.load_dataset = False\n",
    "        self.train_dataset_file = None\n",
    "        self.test_dataset_file = None\n",
    "        self.label2ids = None\n",
    "        self.id2labels = None\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b49dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'O': 0,\n",
    " 'B-Company_Name': 1,\n",
    " 'I-Company_Name': 2,\n",
    " 'B-Internal_Organization': 3,\n",
    " 'I-Internal_Organization': 4,\n",
    " 'B-Software_Name': 5,\n",
    " 'I-Software_Name': 6,\n",
    " 'B-Userbase_Information': 7,\n",
    " 'I-Userbase_Information': 8,\n",
    " 'B-Software_Purpose': 9,\n",
    " 'I-Software_Purpose': 10,\n",
    " 'B-Development_Scalability': 11,\n",
    " 'I-Development_Scalability': 12,\n",
    " 'B-Transaction_Scalability': 13,\n",
    " 'I-Transaction_Scalability': 14,\n",
    " 'B-Data_Scalability': 15,\n",
    " 'I-Data_Scalability': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e2a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'O',\n",
    " 1: 'B-Company_Name',\n",
    " 2: 'I-Company_Name',\n",
    " 3: 'B-Internal_Organization',\n",
    " 4: 'I-Internal_Organization',\n",
    " 5: 'B-Software_Name',\n",
    " 6: 'I-Software_Name',\n",
    " 7: 'B-Userbase_Information',\n",
    " 8: 'I-Userbase_Information',\n",
    " 9: 'B-Software_Purpose',\n",
    " 10: 'I-Software_Purpose',\n",
    " 11: 'B-Development_Scalability',\n",
    " 12: 'I-Development_Scalability',\n",
    " 13: 'B-Transaction_Scalability',\n",
    " 14: 'I-Transaction_Scalability',\n",
    " 15: 'B-Data_Scalability',\n",
    " 16: 'I-Data_Scalability'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6152f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation \n",
    "test_text = os.path.join('kg_tests', 'sigrid_text.words')\n",
    "# test_text = os.path.join(args.datapath,args.dataset, args.test_text)\n",
    "\n",
    "with open(test_text, encoding='utf-8') as f:\n",
    "    test_words = f.readlines()     \n",
    "\n",
    "    \n",
    "# Making dummy tags\n",
    "test_ner_tags = []\n",
    "for t in test_words:\n",
    "    word_list = t.split()\n",
    "    tags = \" \".join([\"O\"] * len(word_list))\n",
    "    tags = tags + \"\\n\"\n",
    "    test_ner_tags.append(tags)\n",
    "\n",
    "\n",
    "# The tokenizer for roberta\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')    \n",
    "    \n",
    "# Getting the labell ids\n",
    "label2ids, id2labels = [], []\n",
    "processed_test_set, test_label_sentence_dicts = [], []\n",
    "\n",
    "label2ids.append(label2id)\n",
    "id2labels.append(id2label)\n",
    "\n",
    "#Keeping track of the unprocessed data\n",
    "unprocessed_test_ner_tags = test_ner_tags\n",
    "unprocessed_test_words = test_words\n",
    "\n",
    "\n",
    "# Processesing data to input into model\n",
    "max_seq_len = args.max_seq_len\n",
    "test_ner_tags, test_words, test_label_sentence_dict = process_data(test_ner_tags, test_words, tokenizer, label2id, max_seq_len,base_model=args.base_model,use_truecase=args.use_truecase)\n",
    "\n",
    "\n",
    "sub_valid_ = [[test_words[i], test_ner_tags[i]] for i in range(len(test_ner_tags))] \n",
    "\n",
    "processed_test_set.append(sub_valid_) \n",
    "\n",
    "\n",
    "dataset_label_nums = [len(x) for x in label2ids]\n",
    "test_num_data_point = sum([len(sub_valid_) for sub_valid_ in processed_test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8283bcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_label_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d24d40cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaNER: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaNER from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaNER from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaNER were not initialized from the model checkpoint at roberta-base and are newly initialized: ['background', 'classifier.weight', 'classifier.bias', 'classifiers.0.weight', 'classifiers.0.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the fine-tuned model\n",
    "new_model = RobertaNER.from_pretrained('roberta-base', dataset_label_nums=dataset_label_nums, output_attentions=False, output_hidden_states=False, multi_gpus=True)\n",
    "new_model = torch.nn.DataParallel(new_model)\n",
    "i = 0\n",
    "new_model.load_state_dict(torch.load(os.path.join(\"trained_model\",f\"FS_train_dataset{i}_finetuned_model_dict.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2131378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predicition from the fine-tuned model\n",
    "def get_predictions(data_):\n",
    "    dataset_chosen = []\n",
    "    data = []\n",
    "    for i,d in enumerate(data_):\n",
    "        one_dataset = [generate_batch(d)]\n",
    "        data.extend(one_dataset)\n",
    "        dataset_chosen.extend([i for x in range(len(one_dataset))])\n",
    "        \n",
    "    idx = 0\n",
    "    f1ss = []\n",
    "    pss = []\n",
    "    rss = []\n",
    "    \n",
    "    \n",
    "    new_model.eval()\n",
    "    \n",
    "    for j, (text, attention_mask, cls, orig_len) in enumerate(data):\n",
    "        with torch.no_grad():\n",
    "            text_1, attention_mask_1, cls_1 = text.to(device), attention_mask.to(device).to(device), cls.to(device)\n",
    "            loss, outputs = new_model(text_1, attention_mask=attention_mask_1, labels=cls_1, dataset = dataset_chosen[j])\n",
    "        preds = [[id2label[int(x)] for j,x in enumerate(y[1:orig_len[i]-1]) if int(cls[i][j + 1]) != -100] for i,y in enumerate(outputs)]\n",
    "        gold = [[id2label[int(x)] for x in y[1:orig_len[i]-1] if int(x) != -100] for i,y in enumerate(cls)]\n",
    "    \n",
    "    return preds, gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25c4b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, original = get_predictions(processed_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f09b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that retireve the indices of an entity when we split the original sentence into a list of tokens\n",
    "def find_entities(labels):\n",
    "    entities = {}\n",
    "    start_idx = None\n",
    "\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label.startswith('B-'):\n",
    "            # Found the start of an entity\n",
    "            if start_idx is not None:\n",
    "                # Add the previous entity to the dictionary\n",
    "                entity = labels[start_idx].split('-')[1]\n",
    "                entities[entity] = entities.get(entity,[]) + [list(range(start_idx, idx))]\n",
    "            \n",
    "            start_idx = idx\n",
    "        elif label.startswith('I-'):\n",
    "            # Continue the current entity\n",
    "            if start_idx is None:\n",
    "                start_idx = idx\n",
    "        else:\n",
    "            # End of entity\n",
    "            if start_idx is not None:\n",
    "                entity = labels[start_idx].split('-')[1]\n",
    "                entities[entity] = entities.get(entity,[]) + [list(range(start_idx, idx))]\n",
    "                start_idx = None\n",
    "\n",
    "    # Check if there's an entity that spans till the end of the list\n",
    "    if start_idx is not None:\n",
    "        entity = labels[start_idx].split('-')[1]\n",
    "        entities[entity] = entities.get(entity,[]) + [list(range(start_idx, len(labels)))]\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb0b6a",
   "metadata": {},
   "source": [
    "# Visualize NER text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2bdf909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b347c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given entity labels\n",
    "entity_labels = ['Transaction_Scalability', 'Software_Purpose', 'Development_Scalability',\n",
    "                 'Userbase_Information', 'Data_Scalability', 'Internal_Organization',\n",
    "                 'Software_Name', 'Company_Name']\n",
    "\n",
    "# Define a list of predefined colors\n",
    "predefined_colors = ['#FFAAAA', '#AAFFAA', '#AAAAFF', '#FFAAEE', '#EEFFAA', '#AAEEFF', '#FFAABB', '#BBAAFF']\n",
    "\n",
    "# Create a dictionary to map entity labels to predefined colors\n",
    "entity_colours = {label: color for label, color in zip(entity_labels, predefined_colors)}\n",
    "\n",
    "# Create the options dictionary with predefined colors\n",
    "options = {\"ents\": list(entity_colours.keys()), \"colors\": entity_colours}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91d6fc0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #FFAABB; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sigrid helps \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Software_Name</span>\n",
       "</mark>\n",
       "you to \n",
       "<mark class=\"entity\" style=\"background: #AAFFAA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    improve your \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Software_Purpose</span>\n",
       "</mark>\n",
       "software by \n",
       "<mark class=\"entity\" style=\"background: #AAFFAA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    measuring \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Software_Purpose</span>\n",
       "</mark>\n",
       "your system’s \n",
       "<mark class=\"entity\" style=\"background: #AAAAFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    code quality, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Development_Scalability</span>\n",
       "</mark>\n",
       "and then \n",
       "<mark class=\"entity\" style=\"background: #AAFFAA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    compares the \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Software_Purpose</span>\n",
       "</mark>\n",
       "results against a benchmark of 10,000 industry systems to give you concrete advice on areas where you can improve.</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #FFAABB; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sigrid performs \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Software_Name</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #AAAAFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    code quality checks \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Development_Scalability</span>\n",
       "</mark>\n",
       "that have been designed by the \n",
       "<mark class=\"entity\" style=\"background: #AAEEFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Software Improvement Group, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Internal_Organization</span>\n",
       "</mark>\n",
       "which have been used by \n",
       "<mark class=\"entity\" style=\"background: #AAEEFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    thousands of development teams \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Internal_Organization</span>\n",
       "</mark>\n",
       "over the past 20 years to help improve their software. \n",
       "<mark class=\"entity\" style=\"background: #BBAAFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SIG’s \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Company_Name</span>\n",
       "</mark>\n",
       "approach is based on the \n",
       "<mark class=\"entity\" style=\"background: #AAAAFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ISO 25010 standard for \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Development_Scalability</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #AAAAFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    software quality, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Development_Scalability</span>\n",
       "</mark>\n",
       "and has been accredited to ensure alignment with the standard.</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In \n",
       "<mark class=\"entity\" style=\"background: #AAAAFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Scrum, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Development_Scalability</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #AAEEFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    developers and \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Internal_Organization</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #AAEEFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    product owners \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Internal_Organization</span>\n",
       "</mark>\n",
       "need to collaborate to prioritize the \n",
       "<mark class=\"entity\" style=\"background: #AAAAFF; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    sprint backlog. \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Development_Scalability</span>\n",
       "</mark>\n",
       "This can be challenging because priorities can be slightly different, or at least perceived differently.</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">At the end of 2020, we had over \n",
       "<mark class=\"entity\" style=\"background: #FFAAEE; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    45,000 customers \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Userbase_Information</span>\n",
       "</mark>\n",
       "located in over \n",
       "<mark class=\"entity\" style=\"background: #FFAAEE; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100 countries, \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Userbase_Information</span>\n",
       "</mark>\n",
       "with \n",
       "<mark class=\"entity\" style=\"background: #FFAAEE; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    millions of users.\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Userbase_Information</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #FFAABB; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Sigrid uses \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Software_Name</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #FFAABB; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon Aurora \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Software_Name</span>\n",
       "</mark>\n",
       "features a distributed, fault-tolerant, \n",
       "<mark class=\"entity\" style=\"background: #EEFFAA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    self-healing storage system \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Data_Scalability</span>\n",
       "</mark>\n",
       "that auto-scales up to \n",
       "<mark class=\"entity\" style=\"background: #EEFFAA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    128TB per database instance. \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Data_Scalability</span>\n",
       "</mark>\n",
       "It delivers \n",
       "<mark class=\"entity\" style=\"background: #FFAAAA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    high performance \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Transaction_Scalability</span>\n",
       "</mark>\n",
       "and availability with up to 15 \n",
       "<mark class=\"entity\" style=\"background: #FFAAAA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    low-latency read \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Transaction_Scalability</span>\n",
       "</mark>\n",
       "replicas, point-in-time recovery, \n",
       "<mark class=\"entity\" style=\"background: #EEFFAA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    continuous backup \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Data_Scalability</span>\n",
       "</mark>\n",
       "to \n",
       "<mark class=\"entity\" style=\"background: #FFAABB; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Amazon S3.\n",
       "\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Software_Name</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize each sentence\n",
    "for i in range(len(pred)):\n",
    "    ner_result = find_entities(pred[i])\n",
    "    tokens = unprocessed_test_words[i].split()\n",
    "    # Initialize the text and ents lists\n",
    "    text = unprocessed_test_words[i]  # Join tokens to create the text string\n",
    "    ents = []\n",
    "\n",
    "    # Iterate through the dictionary and tokens to create entity annotations\n",
    "    for label, indexes in ner_result.items():\n",
    "        for index in indexes:\n",
    "            start = sum(len(tokens[i]) + 1 for i in range(index[0]))\n",
    "            end = start + sum(len(tokens[i]) + 1 for i in range(index[0], index[-1] + 1))\n",
    "            ents.append({\"start\": start, \"end\": end, \"label\": label})\n",
    "    # Create the dic_ents dictionary\n",
    "    dic_ents = {\n",
    "        \"text\": text,\n",
    "        \"ents\": ents,\n",
    "        \"title\": None\n",
    "    }\n",
    "    displacy.render(dic_ents, manual=True, style=\"ent\",options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004371d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
