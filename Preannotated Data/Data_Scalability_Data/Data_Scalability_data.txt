S3 Object Tags are key-value pairs applied to S3 objects which can be created, updated or deleted at any time during the lifetime of the object. With these, you have the ability to create Identity and Access Management (IAM) policies, set up S3 Lifecycle policies, and customize storage metrics. 
S3 Storage Lens delivers organization-wide visibility into object storage usage, activity trends, and makes actionable recommendations to optimize costs and apply data protection best practices. S3 Storage Class Analysis enables you to monitor access patterns across objects to help you decide when to transition data to the right storage class to optimize costs. 
Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides management features so that you can optimize, organize, and configure access to your data to meet your specific business, organizational, and compliance requirements.

DB instances for Amazon RDS for MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server use Amazon Elastic Block Store (Amazon EBS) volumes for database and log storage.
In some cases, your database workload might not be able to achieve 100 percent of the IOPS that you have provisioned.
Amazon RDS provides three storage types: General Purpose SSD (also known as gp2 and gp3), Provisioned IOPS SSD (also known as io1), and magnetic (also known as standard). They differ in performance characteristics and price, which means that you can tailor your storage performance and cost to the needs of your database workload. 
You can create MySQL, MariaDB, Oracle, and PostgreSQL RDS DB instances with up to 64 tebibytes (TiB) of storage.
Provisioned IOPS storage is designed to meet the needs of I/O-intensive workloads, particularly database workloads, that require low I/O latency and consistent I/O throughput. Provisioned IOPS storage is best suited for production environments.
For every DB engine except RDS for SQL Server, you can provision additional IOPS and storage throughput when storage size is at or above the threshold value. For RDS for SQL Server, you can provision additional IOPS and storage throughput for any available storage size. For all DB engines, you pay for only the additional provisioned storage performance. 
If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. To do so, you can use the Amazon RDS console, the Amazon RDS API, or the AWS CLI.
Scaling up database capacity can be a tedious and risky business. Even veteran developers and database administrators who understand the nuanced behavior of their database and application perform this work cautiously. Despite the current era of sharded NoSQL clusters, increasing capacity can take hours, days, or weeks. 
Amazon DynamoDB is a fully managed database that developers and database administrators have relied on for more than 10 years. 
It delivers low-latency performance at any scale and greatly simplifies database capacity management. 
In June 2017, DynamoDB released auto scaling to make it easier for you to manage capacity efficiently, and auto scaling continues to help DynamoDB users lower the cost of workloads that have a predictable traffic pattern. 
Before auto scaling, you would statically provision capacity in order to meet a table’s peak load plus a small buffer. In most cases, however, it isn’t cost-effective to statically provision a table above peak capacity. 
When you create a DynamoDB table, auto scaling is the default capacity setting, but you can also enable auto scaling on any table that does not have it active.
It helps you identify and set up key metrics and logs across your application resources and technology stack, such as database, web (IIS) and application servers, operating system, load balancers, and queues. 
Set up, operate, and scale a managed relational database in the cloud. Although you can set up a database on an EC2 instance, Amazon RDS offers the advantage of handling your database management tasks, such as patching the software, backing up, and storing the backups.

HBase is an open-source, non-relational, distributed database modeled after Google's Bigtable. It was developed as part of Apache Software Foundation's Hadoop project and runs on top of Hadoop Distributed File System (HDFS) to provide BigTable-like capabilities for Hadoop. 
Amazon DynamoDB is a fast, fully-managed NoSQL database service that makes it simple and cost effective to store and retrieve any amount of data, and serve any level of request traffic. DynamoDB helps offload the administrative burden of operating and scaling a highly-available distributed database cluster. This storage alternative meets the latency and throughput requirements of highly demanding applications by providing single-digit millisecond latency and predictable performance with seamless throughput and storage scalability.
Amazon EBS provides two volume types: standard volumes and Provisioned IOPS volumes. They differ in performance characteristics and pricing model, allowing you to tailor your storage performance and cost to the needs of your applications. You can attach and stripe across multiple volumes of either type to increase the I/O performance available to your Amazon EC2 applications. 

The first cloud data lake for enterprises that is secure, massively scalable and built to the open HDFS standard. With no limits to the size of data and the ability to run massively parallel analytics, you can now unlock value from all your unstructured, semi-structured and structured data. 
Apache CouchDB (link resides outside ibm.com) is an open source NoSQL document database that collects and stores data in JSON-based document formats. Unlike relational databases, CouchDB uses a schema-free data model, which simplifies record management across various computing devices, mobile phones, and web browsers.
CouchDB is very customizable and opens the door to developing predictable and performance-driven applications regardless of your data volume or number of users.
If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. 
With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. 
If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. 
EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage).
Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes.
A 30 GB encrypted EBS instance root volume used by the host operating system and Databricks internal services.

Hadoop MapReduce is described as "a software framework for easily writing applications which process vast amounts of data (multi-terabyte data sets) in parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner."
MapReduce filters and sorts data while converting it into key-value pairs. 
By using Spark's distributed computation engine, the package allows users to run large scale data analysis such as selection, filtering, aggregation from R. Karau et al. (2015) provides a summary of the state-of-the-art on using Spark.

When the amount of data in a company reaches a particularly large volume, increases rapidly and includes diverse data formats, this is referred to as a big data scenario. 
When the data volume achieves a magnitude of about 100 TB, specialized and optimized relational database systems reach their architectural and technical limits. As the volume of data increases, so does the effort required to keep the data operationally available and consistent. Relational databases of this size are customized and require cost-intensive hardware.
Storage volumes remain available during this scaling-up operation. 
OpenSearch Service supports 1 EBS volume (max size of 1.5 TB) per instance associated with a domain. With the default maximum of 20 data nodes allowed per OpenSearch Service domain, you can allocate about 30 TB of EBS storage to a single domain. 
If successful, they would like to expand this offering to their consumer line as well, with a much larger volume and a greater market share. 
The expansion of IoT, connected devices and people is generating volumes of data that exceed the storage capacity of any traditional database system. This new type of data is often in formats that are not suitable for storing in relational database tables or for querying using relational query semantics.
But as the volume of data continues to grow exponentially, managing backup and recovery and meeting strict protection service-level objectives (SLOs) has become increasingly challenging. 
How can companies support 10-1000x increases in query and transaction volumes, leverage 50x as much data for decision making, and do everything that used to take hours or days in seconds or fractions of a second?
Its performance for high volume, low latency transactions and data ingestion exceeds the read and write performance and scalability of traditional databases. Ignite can sit on top of all these databases at the same time as an IMDG and coordinate transactions in-memory with the underlying databases to ensure data is never lost.
PayPal, an eBay company, has used Hadoop and other software tools to detect fraud, but the colossal volumes of data were so large their systems were unable to perform the analysis quickly enough.
The problem is, as data volumes grow, querying against a huge, centrally located data set becomes slow and inefficient, and performance suffers. 
One estimate is that 80% of all data today is unstructured; unstructured data is growing 15 times faster than structured data4 and the total volume of data is expected to grow to 40 zettabytes (10^21 bytes) by 2020.
And the number of data engineers sought by companies has recently seen a 96% year-over-year change. But hiring alone is not enough to manage the increase in data volume.
An increasing amount of data is generated and collected across machines, enterprises and applications in unstructured or non-relational format. These data types are characterized not just by the large volumes, but also by their velocity, variety and variability. “Data drifting” is a term that is now commonly used to depict the fluctuation in the format, the pace and the content of data in these new data types.  
Based on continuous observation of resource utilization trends, data-volume processing projections are offered to help with capacity planning. CLAIRE takes this to the next step by offering auto-scaling of data management runtime resources. 
However, high volumes of low cost data on low cost hardware should not be misinterpreted as a signal for reduced service level agreement (SLA) expectations. 
Particularly for SAN performance, some storage vendors say that imposing any type of data layout overhead on the data volume reduces performance.  

One of the features of Data ONTAP that NetApp users consistently comment on is the ability to nondisruptively grow and shrink data volumes as needs change. For example, you can provision a data volume for use with either NAS or SAN protocols and grow it over time to meet changing needs. 
A higher priority gives a volume a greater percentage of available resources when a system is fully loaded.
Analogous to the clustering of multiple database servers in Oracle® Real Application Clusters (Oracle RAC), storage resources across multiple storage controllers can be employed to deliver much greater I/O performance to an application than a single storage controller could achieve alone. 
For example, Leuven University Hospital (UZ Leuven) consolidated all its critical Sybase database storage along with storage used by less critical SQL Server applications on a single set of NetApp storage systems. 
Big Data requires processing high volumes of low-density data, that is, data of unknown value, such as twitter data feeds, clicks on a web page, network traffic, sensor-enabled equipment capturing data at the speed of light, and many more.  It is the task of Big Data to convert low-density data into high-density data, that is, data that has value.  For some companies, this might be tens of terabytes, for others it may be hundreds of petabytes. 
By leveraging Oracle Exadata for your data warehouse, processing can be enhanced with flash memory, columnar databases, in-memory databases, and more. 
Oracle NoSQL Database is designed as a highly scalable, distributed database based on Oracle Berkeley DB. Sleepycat Software. Oracle NoSQL Database is a general purpose, enterprise class key value store that adds an intelligent driver on top of an enhanced distributed Berkeley database. 
Spark or MapReduce processing of high volume, high variety data from multiple data sources and then reduce and optimize dataset to calculate risk profiles.
Its performance for high volume, low latency transactions and data ingestion exceeds the read and write performance and scalability of traditional databases. 
An Ignite cluster can also be used as a distributed, transactional IMDB to support high volume, low latency transactions, and data ingestion, or for low-cost storage.
Kafka on your own, you need to provision servers, configure Apache Kafka manually, replace servers when they fail, orchestrate server patches and upgrades, architect the cluster for high availability, ensure data is durably stored and secured, set up monitoring and alarms, and carefully plan scaling events to support load changes.
It automates most of the common administrative tasks associated with provisioning, configuring, monitoring, backing up, and securing a data warehouse, making it easy and inexpensive to manage and maintain. This automation enables you to build petabyte-scale data warehouses in minutes instead of weeks or months.

Several teams of scientists run complex applications to analyze subsets of those huge volumes of data.
When the volume of data to be analyzed is of the order of terabytes or petabytes (billions of tweets or posts), scalable storage and computing solutions must be used, but no clear solutions today exist for the analysis of Exascale datasets.
Indeed, processing very large data volumes requires operations and new algorithms able to scale in loading, storing, and processing massive amounts of data that generally must be partitioned in very small data grains, on which thousands to millions of simple parallel operations do analysis.
Moving to social media applications, nowadays the huge volume of user-generated data in social media platforms, such as Facebook, Twitter and Instagram, are very precious sources of data from which to extract insights concerning human dynamics and behaviors.
In-memory querying and analytics needed to reduce query response times and execution of analytics operations by caching large volumes of data in the computing node RAMs and issuing queries and other operation in parallel on the main memory of computing nodes.
As Exascale systems are likely to be based on large distributed memory hardware, MPI is one of the most natural programming systems. 
On the other side, we have shared-memory models where the major system is OpenMP that offers a simple parallel programming model although it does not provide mechanisms to explicitly map and control data distribution and includes non-scalable synchronization operations that are making very challenging its implementation on massively parallel systems.
General issues like energy consumption, multitasking, scheduling, reproducibility, and resiliency must be addressed together with other data-oriented issues like data distribution and mapping, data access, data communication and synchronization.
ata locality mechanisms/constructs, like near-data computing must be designed and evaluated on big data applications when subsets of data are stored in nearby processors and by avoiding that locality is imposed when data must be moved. Other challenges concern data affinity control data querying (NoSQL approach), global data distribution and sharing patterns.
In order to resolve the contradiction between requirements of high performance and limited memory resource, we propose a scalable Main-Memory database system ScaMMDB which distributes data and operations to several nodes and makes good use of every node’s resource.
The system must be able to scale with the growth in data size and query volume. For example, it must support trillions of rows and petabytes of data. The update and query performance must hold even as these parameters grow significantly.
Mesa is Google's solution to these technical and operational challenges. Even though subsets of these requirements are solved by existing data warehousing systems, Mesa is unique in solving all of these problems simultaneously for business critical data. Mesa is a distributed, replicated, and highly available data processing, storage, and query system for structured data. Mesa ingests data generated by upstream services, aggregates and persists the data internally, and serves the data via user queries. Even though this paper mostly discusses Mesa in the context of ads metrics, Mesa is a generic data warehousing solution that satisfies all of the above requirements.

Napa: Powering Scalable Data Warehousing with Robust Query Performance at Google.
We need to store and serve these planet-scale data sets under extremely demanding requirements of scalability, sub-second query response times, availability even in the case of entire data center failures, strong consistency guarantees, ingesting a massive stream of updates coming from the applications used around the globe. We have developed and deployed in production an analytical data management system, called Napa, to meet these requirements. 
At its core, Napa’s principal technologies for robust query performance include the aggressive use of materialized views that are maintained consistently as new data is ingested across multiple data centers. Our clients also demand flexibility in being able to adjust their query performance, data freshness, and costs to suit their unique needs. Robust query processing and flexible configuration of client databases are the hallmark of Napa design.
Charged with serving as the Federal lifeline for millions of citizens who need immediate help in the face of life-threatening disasters, the Federal Emergency Management Agency (FEMA) is working with Google Cloud services to run its data management system more efficiently, securely, and collaboratively.
Data unification allows for a single source of information in one repository, creating a shared ecosystem of large amounts of data that users can leverage in real time.
The National Ecological Observatory Network (NEON), the National Institutes of Health (NIH) STRIDES program and NCI Imaging Data Commons are using Google Cloud to help accelerate research productivity with purpose built, scalable data management tools.
NEON was designed for the grand challenges in ecology and Paula Mabee shared how they are partnering with Google to collect and manage over 400 terabytes of raw data per year across 182 data products to accelerate discoveries and help get these important data and knowledge to the into the hands of scientists and decision makers. 
Photon is deployed within Google Advertising System to join data streams such as web search queries and user clicks on advertisements. It produces joined logs that are used to derive key business metrics, including billing for advertisers. Our production deployment processes millions of events per minute at peak with an average end-to-end latency of less than 10 seconds.
