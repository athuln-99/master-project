Amazon EC2 Auto Scaling supports the following types of dynamic scaling policies:
Target tracking scaling — Increase and decrease the current capacity of the group based on a Amazon CloudWatch metric and a target value.
Step scaling—Increase and decrease the current capacity of the group based on a set of scaling adjustments, known as step adjustments, that vary based on the size of the alarm breach.
Simple scaling—Increase and decrease the current capacity of the group based on a single scaling adjustment, with a cooldown period between each scaling activity.
If you are scaling based on a metric that increases or decreases proportionally to the number of instances in an Auto Scaling group, we recommend that you use target tracking scaling policies. Otherwise, we recommend that you use step scaling policies.
With target tracking, an Auto Scaling group scales in direct proportion to the actual load on your application. That means that in addition to meeting the immediate need for capacity in response to load changes, a target tracking policy can also adapt to load changes that take place over time, for example, due to seasonal variations.
When you use an Auto Scaling group without any form of dynamic scaling, it doesn't scale on its own unless you set up scheduled scaling or predictive scaling.
An Auto Scaling group has a maximum capacity of 3, a current capacity of 2, and a dynamic scaling policy that adds 3 instances. When invoking this policy, Amazon EC2 Auto Scaling adds only 1 instance to the group to prevent the group from exceeding its maximum size.
An Auto Scaling group has a minimum capacity of 2, a current capacity of 3, and a dynamic scaling policy that removes 2 instances. When invoking this policy, Amazon EC2 Auto Scaling removes only 1 instance from the group to prevent the group from becoming less than its minimum size.
When the desired capacity reaches the maximum size limit, scaling out stops. If demand drops and capacity decreases, Amazon EC2 Auto Scaling can scale out again.
In this case, Amazon EC2 Auto Scaling can scale out above the maximum size limit, but only by up to your maximum instance weight. Its intention is to get as close to the new desired capacity as possible but still adhere to the allocation strategies that are specified for the group.
An Auto Scaling group has a maximum capacity of 12, a current capacity of 10, and a dynamic scaling policy that adds 5 capacity units.
NoSQL cloud database services, like Amazon DynamoDB, are popular for their simple key-value operations, unbounded scalability and predictable low-latency. Atomic transactions, while popular in relational databases, carry the specter of complexity and low performance, especially when used for workloads with high contention. Transactions often have been viewed as inherently incompatible with NoSQL stores, and the few commercial services that combine both come with limitations. This talk examines the tension between transactions and non-relational databases, and it recounts my journey of adding transactions to DynamoDB.
Amazon Aurora is a MySQL and PostgreSQL compatible relational database engine that combines the speed and availability of high-end commercial databases with the simplicity and cost-eﬀectiveness of open source databases.
Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and availability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and replication across three Availability Zones (AZs).
Amazon Aurora is up to five times faster than standard MySQL databases and three times faster than standard PostgreSQL databases.
Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multiregion, multimaster database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and support peaks of more than 20 million requests per second.
Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. 
Both single-node and up to 15-shard clusters are available, enabling scalability to up to 3.55 TiB of in-memory data.
Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available, and managed Apache Cassandra–compatible database service. 
With Amazon Keyspaces, you can run your Cassandra workloads on AWS using the same Cassandra application code and developer tools that you use today. 
You can build applications that serve thousands of requests per second with virtually unlimited throughput and storage.
Amazon Keyspaces gives you the performance, elasticity, and enterprise features you need to operate business-critical Cassandra workloads at scale.
Amazon MemoryDB for Redis is a Redis-compatible, durable, in-memory database service that delivers ultra-fast performance.
It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. 
Amazon DocumentDB (with MongoDB compatibility)  is a fast, scalable, highly available, and fully managed document database service that supports MongoDB workloads.
Amazon DocumentDB (with MongoDB compatibility) is designed from the ground-up to give you the performance, scalability, and availability you need when operating mission-critical MongoDB workloads at scale. 
Lightsail supports MySQL and PostgreSQL databases , and you can configure them for standard availability for regular workloads or high availability for critical workloads.
Amazon EC2 Auto Scaling helps you ensure that you have the correct number of Amazon EC2 instances available to handle the load for your application. You create collections of EC2 instances, called Auto Scaling groups. You can specify the minimum number of instances in each Auto Scaling group, and Amazon EC2 Auto Scaling ensures that your group never goes below this size.
Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. 
While Amazon S3 is scaling to your new higher request rate, you may see some 503 (Slow Down) errors.
These data lake applications achieve single-instance transfer rates that maximize the network interface use for their Amazon EC2 instance, which can be up to 100 Gb/s on a single instance.
These applications then aggregate throughput across multiple instances to get multiple terabits per second.
Start small and scale as your applications grow with relational databases that are 3-5X faster than popular alternatives, or non-relational databases that give you microsecond to sub-millisecond latency.
AWS fully managed database services provide continuous monitoring, self-healing storage, and automated scaling to help you focus on application development.

Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters.
You can start small for just $0.25 per hour with no commitments and scale out to petabytes of data for $1,000 per terabyte per year, less than a tenth the cost of traditional on-premises solutions.
Amazon Redshift Serverless makes it easier to run and scale analytics without having to manage your data warehouse infrastructure. Developers, data scientists, and analysts can work across databases, data warehouses, and data lakes to build reporting and dashboarding applications, perform near real-time analytics, share and collaborate on data, and build and train machine learning (ML) models.
You can increase or decrease the capacity of the stream at any time according to your business or operational needs, without any interruption to ongoing stream processing. By using API calls or development tools, you can automate scaling of your Amazon Kinesis Data Streams environment to meet demand and ensure you only pay for what you need.
Auto Scaling is a service that enables you to automatically scale your Amazon EC2 capacity up or down according to conditions that you define. With Auto Scaling, you can ensure that the number of EC2 instances you’re using scales up seamlessly during demand spikes to maintain performance, and scales down automatically during demand lulls to minimize costs. 
Each shard gives you a capacity of five read transactions per second, up to a maximum total of 2 MB of data read per second. Each shard can support up to 1,000 write transactions per second, and up to a maximum total of 1 MB data written per second.
With each shard in an Amazon Kinesis stream, you can capture up to 1 megabyte per second of data at 1,000 write transactions per second.
With DynamoDB, you can create database tables that can store and retrieve any amount of data and serve any level of request traffic. You can scale up or scale down your tables' throughput capacity without downtime or performance degradation.
Data management architectures have evolved from the traditional data warehousing model to more complex architectures that address more requirements, such as real-time and batch processing, structured and unstructured data, high velocity transactions, and so on.
Amazon Kinesis Data Streams enables you to choose the throughput capacity you require in terms of shards.
This throughput automatically scales with the number of shards in a stream.
Small scale consistent throughput – Even though Kinesis Data Streams works for streaming data at 200 KB per second or less, it is designed and optimized for larger data throughputs.
Long-term data storage and analytics – Kinesis Data Streams is not suited for long-term data storage. By default, data is retained for 24 hours, and you can extend the retention period by up to 365 days.
This storage alternative meets the latency and throughput requirements of highly demanding applications by providing single-digit millisecond latency and predictable performance with seamless throughput and storage scalability.
With DynamoDB, you can create database tables that can store and retrieve any amount of data and serve any level of request traffic.
You can scale up or scale down your tables' throughput capacity without downtime or performance degradation.
DynamoDB is ideal for existing or new applications that need a flexible NoSQL database with low read and write latencies, and the ability to scale storage and throughput up or down as needed without code changes or downtime.
The underlying hardware is designed for high performance data processing, using local attached storage to maximize throughput between the CPUs and drives, and a 10 GigE mesh network to maximize throughput between nodes. 
This makes it easy for anyone with SQL skills to quickly analyze large-scale datasets.
With Amazon EMR, you can run petabyte-scale analysis at less than half of the cost of traditional on-premises solutions and over 3x faster than standard Apache Spark. 

Azure Monitor supports your operations at scale by helping you maximize the performance and availability of your resources and proactively identify problems.
Integrated monitoring, logging, and trace managed services for applications and systems running on Google Cloud and beyond.
Creating a real-time monitoring system provides accurate and timely decision-making in operational processes.
Databricks, provider of the leading Unified Analytics Platform and founded by the team who created Apache Spark™, today announced that iPass Inc. (NASDAQ: IPAS), a leading provider of global mobile connectivity, is utilizing Databricks’ Unified Analytics Platform and machine learning capabilities to monitor Wi-Fi hotspots in near real-time, and ensure mobile devices are connected to the most accessible hot spot measuring speed, availability, performance, security and location.
Spectrum Conductor offers workload management, monitoring, alerting, reporting and diagnostics and can run multiple current and different versions of Spark and other frameworks concurrently.
This enables users to perform large-scale data transformations and analyses, and then run state-of-the-art machine learning (ML) and AI algorithms.
When data volume rapidly grows, Hadoop quickly scales to accommodate the demand via Hadoop Distributed File System (HDFS). In turn, Spark relies on the fault tolerant HDFS for large volumes of data.

Load balancing enables scalability, avoids bottlenecks and also reduces time taken to give the respond. Many load balancing algorithm [2] have been designed in order to schedule the load among various machines. But so far there is no such ideal load balancing algorithm has been developed which will allocate the load evenly across the system. 
This simply means that the software intelligent load balancers are also used to provide actionable insights to an organization. In this section, we look at how routing is done in SDN to facilitate for intelligent load balancing. In order to appreciate the power of intelligent load balancing routing in SDN and its advantages, we first cover a summary of load balancing routing in IP networks.
Load unbalancing problem is a multi-variant, multi-constraint problem that degrades performance and efficiency of computing resources. Load balancing techniques cater the solution for load unbalancing situation for two undesirable facets- overloading and under-loading.
Load balancing is the process of redistribution of workload in a distributed system like cloud computing ensuring no computing machine is overloaded, under-loaded or idle [12, 13]. Load balancing tries to speed up different constrained parameters like response time, execution time, system stability etc. thereby improving performance of cloud [14, 15]. It is an optimization technique in which task scheduling is an NP hard problem. There are a large number of load balancing approaches proposed by researchers where most of focus has been concerned on task scheduling, task allocation, resource scheduling, resource allocation, and resource management.
Recent advances in programmable data planes, software-defined networking, and the adoption of IPv6, support novel, more complex load balancing strategies. 
It has sustained the rapid global growth of Google services, and it also provides network load balancing for Google Cloud Platform.
We’ll discuss Google Cloud Load Balancer (GCLB) as a concrete example of large-scale load balancing, but nearly all of the best practices we describe also apply to other cloud providers’ load balancers.
Autoscale using a capacity metric as observed by the load balancer. This will automatically discount unhealthy instances from the average.
If your system becomes sufficiently complex, you may need to use more than one kind of load management. For example, you might run several managed instance groups that scale with load but are cloned across multiple regions for capacity; therefore, you also need to balance traffic between regions. In this case, your system needs to use both load balancing and load-based autoscaling.
If your site gets popular on social media and suddenly experiences a five-fold increase in traffic, you’d prefer to serve what requests you can. Therefore, you implement load shedding to drop excess traffic. In this case, your system needs to use both load balancing and load shedding.
Load balancing, load shedding, and autoscaling are all systems designed for the same goal: to equalize and stabilize the system load. 
Dressy’s development teams investigate and notice a problem: their load balancing is inexplicably drawing all user traffic into region A, even though that region is full-to-overflowing and both B and C are empty (and equally large). 
In brief, the load balancer didn’t know that the “efficient” requests were errors because the load shedding and load balancing systems weren’t communicating. Each system was added and enabled separately, likely by different engineers. No one had examined them as one unified load management system.
Load balancing minimizes latency by routing to the location closest to the user. Autoscaling can work together with load balancing to increase the size of locations close to the user and then route more traffic there, creating a positive feedback loop.
Autoscaling is a powerful tool, but it’s easy to get wrong. Unless carefully configured, autoscaling can result in disastrous consequences—for example, potentially catastrophic feedback cycles between load balancing, load shedding, and autoscaling when these tools are configured in isolation. As the Pokémon GO case study illustrates, traffic management works best when it’s based upon a holistic view of the interactions between systems.
Time and time again, we’ve seen that no amount of load shedding, autoscaling, or throttling will save our services when they all fail in sync.
Several techniques have been reported in the literature to improve performance and resource use based on load balancing, task scheduling, resource management, quality of service, and workload management. Load balancing in the cloud allows data centers to avoid overloading/underloading in virtual machines, which itself is a challenge in the field of cloud computing. Therefore, it becomes a necessity for developers and researchers to design and implement a suitable load balancer for parallel and distributed cloud environments. 
Currently, load balancing in the cloud (LBC) is one of the main challenges that allows avoiding the situation of overloading/underloading in virtual machines during task computation.
Thus, there is a need to identify the issues that affect LBC and develop an effective load balancing technique for cloud environments.
Load balancing provides the facility to distribute the workload equally on available resources. Its objective is to provide continuous service in case of failure of any service’s component by provisioning and deprovisioning the application instances along with proper utilization of resources.
Load balancer helps in allocation of resources to the tasks fairly for resource utilization and user satisfaction at minimum cost, which motivates us to find issues in load balancing and to work on resolving them.
Along with
load balancing, there are various challenges, such as resource scheduling, performance monitoring, QoS management, energy consumption, and service availability in the cloud.

OLTP or Online Transaction Processing is a type of data processing that consists of executing a number of transactions occurring concurrently—online banking, shopping, order entry, or sending text messages, for example. These transactions traditionally are referred to as economic or financial transactions, recorded and secured so that an enterprise can access the information anytime for accounting or reporting purposes.
As IT struggles to keep pace with the speed of business, it is important that when you choose an operational database you consider your immediate data needs and long-term data requirements.
For storing transactions, maintaining systems of record, or content management, you will need a database with high concurrency, high throughput, low latency, and mission-critical characteristics such as high availability, data protection, and disaster recovery.
Also, if your data needs grow and you want to expand the functionality of your application, adding more single-purpose or fit-for-purpose databases will only create data silos and amplify the data management problems.
You must also consider other functionalities that may be necessary for your specific workload—for example, ingestion requirements, push-down compute requirements, and size at limit.
Select a future-proof cloud database service with self-service capabilities that will automate all the data management so that your data consumers—developers, analysts, data engineers, data scientists and DBAs—can do more with the data and accelerate application development.
They had to evolve to handle the modern-day transactions, heterogeneous data, and global scale, and most importantly to run mixed workloads. Relational databases transformed into multimodal databases that store and process not only relational data but also all other types of data, including xml, html, JSON, Apache Avro and Parquet, and documents in their native form, without much transformation.
The choice depends heavily on your use case — transactional processing, analytical processing, in-memory database, and so on — but it also depends on other factors. This post covers the different database options available within Google Cloud across relational (SQL) and non-relational (NoSQL) databases and explains which use cases are best suited for each database option. 
Provides managed MySQL, PostgreSQL and SQL Server databases on Google Cloud. It reduces maintenance cost and automates database provisioning, storage capacity management, back ups, and out-of-the-box high availability and disaster recovery/failover.
Non-relational databases are often used when large quantities of complex and diverse data need to be organized, or where the structure of the data is regularly evolving to meet new business requirements. Unlike relational databases, they perform faster because a query doesn’t have to access several tables to deliver an answer, making them ideal for storing data that may change frequently or for applications that handle many different kinds of data. 
Indeed, at times too much information might overwhelm the user, threatening to saturate their workload capacity, a cognitive mechanism of limited size that distributes some resources, such as working memory, to cognitive processes as required.
Minimising the amount of cognitive resources spent on this cycle has the potential to decrease the imposition of the UI on the user’s workload capacity.
Workload capacity is the construct we have used to refer to the cognitive mechanism underpinning multitasking.
Our previous research using test runs, execution time, and test input information for reliability analysis and improvement is extended to ensure better test workload measurements for reliability assessment and prediction.
However, benchmarking and comparing the energy efficiency of GPGPU workloads is challenging as standardized workloads are rare and standardized power and efficiency measurement methods and metrics do not exist. In addition, not all GPGPU systems run at maximum load all the time. Systems that are utilized in transactional, request driven workloads, for example, can run at lower utilization levels. Existing benchmarks for GPGPU systems primarily consider performance and are intended only to run at maximum load. 

Cloud computing is popular in industry due to its ability to deliver on-demand resources according to a pay-as-you-go model.
Generally, the providers implement an automatic provisioning approach via the virtualization technique. Virtualization makes it possible to rapidly scale the resources up or down. The aforementioned approaches present a reactive method, which is triggered by a certain threshold, such as CPU utilization or memory utilization. Actually, two or more thresholds should be used as a performance metric.
First we propose the elastic resource provisioning (ERP) approach on the performance threshold. 
Thus, combining this with an automatic method and a proactive method would be more agile for provisioning the resources. For example, the Elastic VM architecture provisions the resources dynamically to reduce the SLA violation. However, the elasticity is necessary to meet the users’ demand from different perspectives.
To solve the mentioned issues, we propose the ERP approach to provision the resources by the performance threshold, including the CPU and the memory. According to the threshold, we would flexibly scale the resources up or down by considering multiple perspectives. From the perspective of the provider, the goal is aimed at minimizing the amount of the resources to reduce the energy consumption. From the perspective of the users, the goal is aimed at rapidly scaling the resources up or down.
It presents a cost-efficient method to scale up from the perspective of the providers. In contrast, our approach considers more factors to formulate the threshold by the cloud layer model, such as CPU utilization, memory utilization, etc. Additionally, we aim to scale the resources by minimizing the renting cost and response time.
Therefore, it is important to scale the resources from different granularities, including horizontal elasticity and vertical elasticity. 
In fact, elasticity is essential to meet a fluctuating workload, and it is necessary to determine the suitable amount of the resources in order to scale the resources.
CloudScale is a system that automates the fine-grained resources in cloud computing infrastructures, determining the adaptive resources by the prediction. 
It implements an elastic resource provisioning approach in the datacenter. This algorithm takes the performance threshold as the baseline to scale the resources up or down. 
Scalability means to the ability of the system to deal with an increasing amount of the servers in a capable manner. 
In the automatic policy, the resources would be provisioned and released automatically according to the demand. Generally, the action is triggered by the fixed thresholds, such as the utilization. The common techniques are provided by Amazon and Scalr. However, they provision the resources only based on the utilization, when in fact more elements have taken effect.
PRESS is a predictive elasticity system that analyzes and extracts the workload patterns and provisions the resources automatically. 
Automated resource provisioning techniques enable the implementation of elastic services, by adapting the available resources to the service demand. This is essential for reducing power consumption and guaranteeing QoS and SLA fulfillment, especially for those services with strict QoS requirements in terms of latency or response time, such as web servers with high traffic load, data stream processing, or real-time big data analytics. Elasticity is often implemented in cloud platforms and virtualized data-centers by means of auto-scaling mechanisms. These make automated resource provisioning decisions based on the value of specific infrastructure and/or service performance metrics. 
On the other hand, service elasticity enables power consumption to be reduced, by avoiding resource over-provisioning. 
The auto-scaling mechanisms should allow the system to dynamically adapt to workload changes, by autonomously provisioning and de-provisioning resources (i.e., back-end servers), so that at each point in time, the available resources match the current service demand as closely as possible.
Most control-based systems are reactive mechanisms, for example Lim et al. [30] propose extending the cloud platform with an external feedback controller that enable users to automate the resource provisioning, and introduce the concept of proportional thresholding, a new control policy that takes into account the coarse-grained actuators provided by resource providers.
Deciding where to handle services and tasks, as well as provisioning an adequate amount of computing resources for this handling, is a main challenge of edge computing systems.
We propose the concept of spare edge device to handle dynamic load changes in an elastic way, as well as algorithms for provisioning these devices with different QoS/cost tradeoffs.
In contrast to existing works, we propose a non-demand elastic resource provisioning to minimize the overall power consumption of the network (i.e., joint power consumption of the cell sites and VBS pool) while maximizing the resource utilization.

Similar scalability is observed in the sequential read and write workloads. Note that the Maximum Transfer Unit (MTU) was set as 9000 to the Virtual SAN network interfaces to get maximum performance for the two disk group configuration for the All Read workload. This is mainly to reduce the CPU utilization consumed by the vSphere network stack at such high loads. 
Setting a higher MTU (for example, 9000) may help to get maximum performance for all cached read workloads when using more than one disk group. 
The second impact of removing the read cache is that workload performance should stay steady as the working set size is increased beyond the size of the “caching” tier SSD.
This workload can be used to understand the maximum random read I/Os per second (IOPS) that a storage solution can deliver. 
In the All Read and Mixed R/W experiments, there are two important metrics to follow: I/Os per second (IOPS) and the mean latency encountered by each I/O operation. 
The number of outstanding I/Os is 128 per VM for the All Read workload, and 32 per VM for the Mixed R/W workload.
The ECM Workload was executed regularly throughout the population, scaling to over 120,000 transactions per minute.
The Performance Efficiency pillar includes the ability to use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve.
Cloud storage is typically more reliable, scalable, and secure than traditional on-premises storage systems.

